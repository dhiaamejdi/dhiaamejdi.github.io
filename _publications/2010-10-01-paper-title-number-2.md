---
title: "DE-Bench: A Comprehensive Benchmark for Evaluating Large Language Models on Data Engineering Tasks"
collection: publications
category: manuscripts
permalink: /publication/2025-05-01-de-bench
excerpt: 'A benchmark for assessing LLM performance on SQL optimization, data quality, schema matching, and pipeline generation.'
date: 2025-05-01
venue: 'Manuscript in preparation Target Venue: NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle'
paperurl: 'https://drive.google.com/file/d/1Y-rRXafIiKX_qZcP9IHJoRs1vD-5Qidz/view?usp=sharing'
citation: 'Dhiaa Mejdi*. (2025). "DE-Bench: A Comprehensive Benchmark for Evaluating Large Language Models on Data Engineering Tasks." NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle.'
---
We introduce DE-Bench, a novel benchmark designed to evaluate the performance of Large Language Models (LLMs) on critical data engineering tasks. Our benchmark encompasses four key domains: SQL optimization, data quality assessment, schema matching, and pipeline generation. We evaluate five widely-used models including GPT-2, LLaMA-2-7B, Mistral-7B, Falcon-7B, and GPT-3.5-Turbo across these tasks using quantized implementations for efficient evaluation. Our results reveal significant performance variations across models and tasks, with open-source 7B parameter models achieving comparable or superior performance to larger commercial models on specific data engineering tasks. The benchmark and evaluation framework are made publicly available to facilitate future research in LLM applications for data engineering.
